{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Transfer Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vcpi_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAULA 10/util\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvcpi_util\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vcpi_util'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# importing a module with utilities for displaying stats and data\n",
    "import sys\n",
    "sys.path.insert(1, './util')\n",
    "import vcpi_util\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, loss_fn, optimizer, scheduler, early_stopper, save_prefix = 'model'):\n",
    "\n",
    "    history = {}\n",
    "    history['accuracy'] = []\n",
    "    history['val_acc'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['loss'] = []\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        model.train()\n",
    "        start_time = time.time() \n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in tqdm(enumerate(train_loader, 0)):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.cpu().detach().numpy()\n",
    "            correct += (predicted == targets).sum().cpu().detach().numpy()\n",
    "\n",
    "        model.eval()\n",
    "        v_correct = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i,t in val_loader:\n",
    "                i = i.to(device)\n",
    "                t = t.to(device)\n",
    "                o = model(i)\n",
    "                _,p = torch.max(o,1)\n",
    "                \n",
    "                #with torch.no_grad():\n",
    "                val_loss += loss_fn(o, t).cpu().detach().numpy()\n",
    "\n",
    "                v_correct += (p == t).sum().cpu().detach().numpy()\n",
    "\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if old_lr != new_lr:\n",
    "            print('==> Learning rate updated: ', old_lr, ' -> ', new_lr)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        accuracy = 100 * correct / len(train_loader.dataset)\n",
    "        v_accuracy = 100 * v_correct / len(val_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        stop_time = time.time()\n",
    "        print(f'Epoch: {epoch:03d}; Loss: {epoch_loss:0.6f}; Accuracy: {accuracy:0.4f}; Val Loss: {val_loss:0.6f}; Val Acc: {v_accuracy:0.4f}; Elapsed time: {(stop_time - start_time):0.4f}')\n",
    "        history['accuracy'].append(accuracy)\n",
    "        history['val_acc'].append(v_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['loss'].append(epoch_loss)\n",
    " \n",
    "        ###### Saving ######\n",
    "        if val_loss < best_val_loss:\n",
    "           \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model':model.state_dict(),\n",
    "                'history': history,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "                },\n",
    "                f'{save_prefix}_best.pt')\n",
    "\n",
    "        if early_stopper(val_loss):\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "    return(history)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "\n",
    "    # sets the model in evaluation mode.\n",
    "    # although our model does not have layers which behave differently during training and evaluation\n",
    "    # this is a good practice as the models architecture may change in the future\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "         \n",
    "        # forward pass, compute the output of the model for the current batch\n",
    "        outputs = model(images.to(device))\n",
    "\n",
    "        # \"max\" returns a namedtuple (values, indices) where values is the maximum \n",
    "        # value of each row of the input tensor in the given dimension dim; \n",
    "        # indices is the index location of each maximum value found (argmax).\n",
    "        # the argmax effectively provides the predicted class number        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct += (preds.cpu() == targets).sum()\n",
    "\n",
    "    return (correct / len(data_loader.dataset)).item()\n",
    "\n",
    "\n",
    "class Early_Stopping():\n",
    "\n",
    "    def __init__(self, patience = 3, min_delta = 0.00001):\n",
    "\n",
    "        self.patience = patience \n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.min_delta\n",
    "        self.min_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        # improvement\n",
    "        if val_loss + self.min_delta < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "        # no improvement            \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def build_confusion_matrix(model, dataset):\n",
    "\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "\n",
    "    for images, targets in dataset:\n",
    "\n",
    "        predictions = model(images.to(device))\n",
    "        preds_sparse = [np.argmax(x) for x in predictions.cpu().detach().numpy()]\n",
    "        preds.extend(preds_sparse)\n",
    "        ground_truth.extend(targets.numpy())\n",
    "\n",
    "    vcpi_util.show_confusion_matrix(ground_truth, preds, len(test_set.classes))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "SAMPLES = 23000\n",
    "\n",
    "prefix = 'dogsVScats/dogsVScats'\n",
    "\n",
    "PATH_TEST_SET = f'{prefix}/test'\n",
    "PATH_VAL_SET =f'{prefix}/val{SAMPLES}'\n",
    "\n",
    "PATH_TRAINING_SET = f'{prefix}/train{SAMPLES}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset: Dogs Vs Cats\n",
    "\n",
    "Original data set can be found in: https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "![image](dogs_vs_cats.png)\n",
    "\n",
    "The dataset consists of 25000 thousand images from cats and dogs. 2000 images were randomly collected for the test set. From the remaining 23000 images, 20% were used for the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMG_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m      3\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m v2\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m----> 4\u001b[0m     v2\u001b[38;5;241m.\u001b[39mResize((\u001b[43mIMG_SIZE\u001b[49m,IMG_SIZE)),\n\u001b[0;32m      5\u001b[0m     v2\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m      6\u001b[0m     v2\u001b[38;5;241m.\u001b[39mToImage(),\n\u001b[0;32m      7\u001b[0m     v2\u001b[38;5;241m.\u001b[39mToDtype(torch\u001b[38;5;241m.\u001b[39mfloat32, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      8\u001b[0m     v2\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],[\u001b[38;5;241m0.229\u001b[39m,\u001b[38;5;241m0.224\u001b[39m,\u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m      9\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMG_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
